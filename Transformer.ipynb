{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from random import randint\n",
    "import random\n",
    "\n",
    "def fetch():\n",
    "    \"\"\"\n",
    "    emit a three token sequence, where each token is 4 dim. \n",
    "    \n",
    "    only the first token has any predictive value, since y is 1 if token 1 \n",
    "    is [1,1,1,1] and 0 if token 1 is [-1,-1,-1,-1].\n",
    "    \"\"\"\n",
    "    a = np.array(random.choice([[1, 1, 1, 1], [-1, -1, -1, -1]]))\n",
    "    b = np.random.uniform(-.5, .5, 4)\n",
    "    c = np.random.uniform(-.5, .5, 4)\n",
    "        \n",
    "    y = (a[0] == 1)\n",
    "    return ([a, b, c], int(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# section 1: baseline\n",
    "we have a sentence consisting of three tokens, each token is a 4-dim embedding. each sentence is designed such that only the first token has any predictive value - that is, whether y=1 or y=0 depends only on the first token, which can only be [1,1,1,1] or [-1,-1,-1,-1].\n",
    "\n",
    "to construct the baseline, we fit a simple linear model to token 2 only. since token 2 is randomly generated (from a uniform distribution of [-.5, .5]) it should have no predictive value. we should expect this neural net to get 50% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanilla_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vanilla_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=4, out_features=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        y_prob = F.softmax(x, dim=1)[0]\n",
    "        return y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_net = Vanilla_Net()\n",
    "baseline_opt = optim.SGD(baseline_net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.46\n",
      "accuracy 0.5\n",
      "accuracy 0.53\n",
      "accuracy 0.47\n",
      "accuracy 0.47\n",
      "accuracy 0.5\n",
      "accuracy 0.55\n",
      "accuracy 0.46\n",
      "accuracy 0.43\n",
      "accuracy 0.55\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # train\n",
    "    for _ in range(300):\n",
    "        \n",
    "        # generate data, we only want to train on token 2\n",
    "        X_train, y_train = fetch()\n",
    "        token2 = torch.tensor(list(X_train[1])).reshape(1,4)\n",
    "\n",
    "        baseline_opt.zero_grad()\n",
    "        outputs = baseline_net(token2)\n",
    "        loss = F.nll_loss(outputs.reshape(1, 2), torch.tensor([y_train]))\n",
    "        loss.backward()\n",
    "        baseline_opt.step()\n",
    "        \n",
    "    # test\n",
    "    acc = 0\n",
    "    for _ in range(100):\n",
    "        # generate data, we only want to train on token 2\n",
    "        X_test, y_test = fetch() \n",
    "        token2 = torch.tensor(list(X_test[1])).reshape(1,4)\n",
    "        outputs = baseline_net(token2)\n",
    "        pred = int(torch.argmax(outputs))\n",
    "        if pred == y_test:\n",
    "            acc += 1\n",
    "    print(\"accuracy\", acc/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[example](http://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# section 2: transformers\n",
    "we will still train on token2, which we know has no predictive value. \n",
    "\n",
    "however, in this case, we will transform token2 before passing it to a simple linear network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.query_matrix = nn.Parameter(torch.randn(4, 3))\n",
    "        self.key_matrix = nn.Parameter(torch.randn(4, 3))\n",
    "        self.fc1 = nn.Linear(in_features=4, out_features=2)  # modified token2 is still dim=4\n",
    "        \n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        # input_seq has 3 tokens, each token a 4 dim vector\n",
    "        \n",
    "        # we only want token2\n",
    "        token2 = torch.FloatTensor([input_seq[1]])\n",
    "            \n",
    "        # generate its query vector from query matrix\n",
    "        q = torch.mm(token2, self.query_matrix)\n",
    "\n",
    "        # generate an attention mask\n",
    "        scores = []  # list of scores, where each element is s = q * k\n",
    "\n",
    "        # compute similarity scores by multiplying query vector by key vector of every token in the sentence\n",
    "        for t in input_seq:\n",
    "            token_i = torch.FloatTensor([t])  # convert python list to torch tensor\n",
    "\n",
    "            # create a key for each token by multiplying it by the key matrix\n",
    "            k = torch.mm(token_i, self.key_matrix)\n",
    "\n",
    "            # calculate similarity score for each token2/token_i pair\n",
    "            s = torch.mm(q, k.reshape(3, 1))  # s is a scalar representing the degree of similarity\n",
    "            scores.append(s[0][0])\n",
    "        \n",
    "        # softmax to create an attention mask\n",
    "        mask = F.softmax(torch.stack(scores), dim=0)\n",
    "\n",
    "        # multiply each token in the sentence by its corresponding score in the mask\n",
    "        new_input_seq = []  # generate a new 3 token sentence, where each token is 4dim\n",
    "        for pair in zip(input_seq, mask):\n",
    "            new_embedding = torch.FloatTensor(pair[0]) * pair[1]\n",
    "            new_input_seq.append(new_embedding)\n",
    "        new_input_seq = (torch.stack(new_input_seq))  # recast python list to torch tensor\n",
    "        modified_token2 = torch.sum(new_input_seq, dim=0).reshape(1, 4)  # new embeddeding for token2\n",
    "\n",
    "        # predict the modified token 2\n",
    "        x = self.fc1(modified_token2)\n",
    "        y_prob = F.softmax(x, dim=1)[0]\n",
    "        return y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_net = Transformer()\n",
    "transformer_net_opt = optim.SGD(transformer_net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93\n",
      "0.98\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # train\n",
    "    for _ in range(300):\n",
    "        X_train, y_train = fetch()\n",
    "        transformer_net_opt.zero_grad()\n",
    "        outputs = transformer_net(X_train)\n",
    "        loss = F.nll_loss(outputs.reshape(1, 2), torch.tensor([y_train]))\n",
    "        loss.backward()\n",
    "        transformer_net_opt.step()\n",
    "\n",
    "    # test\n",
    "    acc = 0\n",
    "    for _ in range(100):\n",
    "        X_test, y_test = fetch() \n",
    "        outputs = transformer_net(X_test)\n",
    "        pred = int(torch.argmax(outputs))\n",
    "        if pred == y_test:\n",
    "            acc += 1\n",
    "    print(acc/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
