{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def fetch(k):\n",
    "    \"\"\"\n",
    "    emit a sequence of three tokens, where each token has a dimension = k.\n",
    "    \n",
    "    only the first token has any predictive value, since y is 1 if token 1 \n",
    "    is [1,1,1,1] and 0 if token 1 is [-1,-1,-1,-1].\n",
    "    \"\"\"\n",
    "    a = np.array(random.choice([[1] * k, [-1] * k]))\n",
    "    b = np.random.uniform(-1, 1, k)\n",
    "    c = np.random.uniform(-1, 1, k)\n",
    "    d = np.array(random.choice([[1] * k, [-1] * k]))\n",
    "        \n",
    "    y = (a[0] != d[0])  # xor, where y = 1 if only one input is equal to 1\n",
    "    return ([a, b, c, d], int(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# section 1: baseline\n",
    "we have a sentence consisting of three tokens, each token is a 4-dim embedding. \n",
    "\n",
    "each sentence is designed so that only the first token has any predictive value - that is, whether `y=1` or `y=0` depends only on the first token, which can only be `[1,1,1,1]` or `[-1,-1,-1,-1]`.\n",
    "\n",
    "therefore, when we fit a simple linear model to token 2 (generated from a uniform distribution of `[-1, 1]`), we should not be able to do better than random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanilla_Net(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(Vanilla_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=embedding_dim, out_features=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        prob = F.softmax(x, dim=1)[0]\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3  # dimensionality of each token\n",
    "\n",
    "baseline_net = Vanilla_Net(embedding_dim=k)\n",
    "baseline_opt = optim.SGD(baseline_net.parameters(), lr=1e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # train\n",
    "    for _ in range(500):\n",
    "        \n",
    "        # generate data, we only want to train on token 2\n",
    "        X, y = fetch(k)\n",
    "        token2 = torch.tensor(list(X[1])).reshape(1,k)\n",
    "\n",
    "        baseline_opt.zero_grad()\n",
    "        outputs = baseline_net(token2)\n",
    "        loss = F.nll_loss(outputs.reshape(1, 2), torch.tensor([y]))\n",
    "        loss.backward()\n",
    "        baseline_opt.step()\n",
    "        \n",
    "    # test\n",
    "    acc = 0\n",
    "    for _ in range(100):\n",
    "        X, y = fetch(k)\n",
    "        token2 = torch.tensor(list(X[1])).reshape(1,k)\n",
    "        outputs = baseline_net(token2)\n",
    "        pred = int(torch.argmax(outputs))\n",
    "        if pred == y:\n",
    "            acc += 1\n",
    "    print(\"accuracy\", acc/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# section 2: transformers\n",
    "we will still train on token2, which has no predictive value. \n",
    "\n",
    "however, we will modify token2 before passing it to a neural net. \n",
    "\n",
    "first, we compare token2 (represented by a query vector) to every other token (represented by a key vector) in the sentence. the query vector is computed by multiplying token2 by a matrix Q. key vectors are computed by multiplying every token in the sentence by a matrix K.\n",
    "\n",
    "we take the inner product of the query vector and key vector, for all key vectors. this creates a 1-dim array of length K, where K is number of tokens in the sentence, where each element contains the inner product (which implicitly measures the similarity score). this \"score\" vector is then softmaxed to create an attention mask. \n",
    "\n",
    "finally, the attention mask is applied to every token in the sentence. the attention mask can be interpreted as a weighting mechanism, weighting all of the tokens in the sentence. the weighted token embeddings are summed up to create a new token2, which now contains mixtures of all other tokens in the sentence. \n",
    "\n",
    "in order to reduce loss, the neural wants a modified token2 that has high predictive value, which in turn will want a softmax that places high weights to other tokens with high predictive value (and low weights to mask out tokens with no predictive value). therefore, the query matrix and key matrix learns to how to maximize the inner product between token2 and tokens with high predictive value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.query_matrix = nn.Parameter(torch.randn(embedding_dim, 3))\n",
    "        self.key_matrix = nn.Parameter(torch.randn(embedding_dim, 3))\n",
    "        self.fc1 = nn.Linear(in_features=embedding_dim, out_features=2)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        # sentence has 3 tokens, each token is a k-dim embedding\n",
    "        \n",
    "        # we only want token2, which by itself has no predictive value\n",
    "        token2 = torch.FloatTensor([sentence[1]])\n",
    "            \n",
    "        # generate token2's query vector by multiplying token2 with the query matrix\n",
    "        q = torch.mm(token2, self.query_matrix)\n",
    "\n",
    "        # generate an attention mask\n",
    "        scores = []  # list of scores, where each element is s = q * k\n",
    "\n",
    "        # compute similarity scores by multiplying query vector by every possible key vector\n",
    "        for t in sentence:\n",
    "            token_i = torch.FloatTensor([t])  # convert python list to torch tensor\n",
    "\n",
    "            # create key vectors by multiplying each token by the key matrix\n",
    "            k = torch.mm(token_i, self.key_matrix)\n",
    "\n",
    "            # calculate similarity score by taking the inner product of query vector with each key vector\n",
    "            s = torch.mm(q, k.reshape(3, 1))  # s is a scalar representing the degree of similarity\n",
    "            scores.append(s[0][0])\n",
    "        \n",
    "        # softmax to create an attention mask\n",
    "        mask = F.softmax(torch.stack(scores), dim=0)\n",
    "\n",
    "        # multiply each token in the sentence by its corresponding score in the mask\n",
    "        ls_weighted_embeddings = []  # generate a new 3 token sentence, where each token is 4dim\n",
    "        for pair in zip(sentence, mask):\n",
    "            new_embedding = torch.FloatTensor(pair[0]) * pair[1]  # weight each embedding by the mask\n",
    "            ls_weighted_embeddings.append(new_embedding)\n",
    "        ls_weighted_embeddings = (torch.stack(ls_weighted_embeddings))  # recast python list to torch tensor\n",
    "        modified_token2 = torch.sum(ls_weighted_embeddings, dim=0).reshape(1, self.embedding_dim)  # new embedding for token2\n",
    "\n",
    "        # learn using the modified embedding for token2\n",
    "        x = self.fc1(modified_token2)\n",
    "        prob = F.softmax(x, dim=1)[0]\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4  # dimensionality of each token\n",
    "transformer_net = Transformer(embedding_dim=k)\n",
    "transformer_net_opt = optim.SGD(transformer_net.parameters(), lr=1e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # train\n",
    "    for _ in range(3000):\n",
    "        X, y = fetch(k)\n",
    "        transformer_net_opt.zero_grad()\n",
    "        outputs = transformer_net(X)\n",
    "        loss = F.nll_loss(outputs.reshape(1, 2), torch.tensor([y]))\n",
    "        loss.backward()\n",
    "        transformer_net_opt.step()\n",
    "\n",
    "    # test\n",
    "    acc = 0\n",
    "    for _ in range(100):\n",
    "        X, y = fetch(k) \n",
    "        outputs = transformer_net(X)\n",
    "        pred = int(torch.argmax(outputs))\n",
    "        if pred == y:\n",
    "            acc += 1\n",
    "    print(acc/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
